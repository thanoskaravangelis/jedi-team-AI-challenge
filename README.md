# GWI â€“ Jedi Team â€“ AI Engineering Challenge

Welcome to the **AI engineering** challenge for the Jedi Team at GWI!

This task is designed to help us understand how you approach appliedâ€AI problems and build intelligent, productionâ€‘ready systems. The Jedi team owns and evolves the companyâ€™s AI infrastructure, so the exercise focuses on agentic patterns, tool orchestration, and reasoning quality. Creativity, thoughtful design, and clean code are all appreciated.

---

## ğŸ§ª Core Scenario

GWIâ€™s marketâ€‘research data has been converted into naturalâ€‘language statements and stored in a relational database. A sample export lives in `data.md`.

Your mission is to implement an **agentic chatbot** that helps clients answer questions using that dataâ€”and, when needed, additional tools.

### The agent should be able to

1. **Retrieve answers from internal data**  
   * Exact match or similarity search is acceptable.  
   * Cite the data fragment you used.

2. **Plan a course of action** when the answer is not immediately available.  
   * E.g. â€œI will check the database; if that fails, Iâ€™ll run a web search.â€

3. **Call at least two external tools** (examples, choose any two or invent your own):  
   * Webâ€‘search API.  
   * Vectorâ€‘database or RAG lookup.  
   * Classifier to ensure quality of the answer.

4. **Evaluate and Score Agent Responses**  
   * Implement an evaluation process to assess the quality and relevance of the agent's answers.  
   * Use the evaluation results to iteratively improve the agent's reasoning and tool selection.

5. **Finetune the Agent Based on Feedback**  
   * Incorporate user feedback and evaluation outcomes to continuously finetune the agent's models or decision logic.  
   * This may involve retraining components, updating prompts, or refining tool selection strategies to enhance performance over time.

6. **Persist conversations** so a user can resume from where they left off.  
   * Support multiple concurrent chats per user.

7. **Expose an HTTP interface** (REST, SSE, or WebSocket) that allows a UI to stream the agentâ€™s intermediate reasoning (â€œthoughtsâ€) and final answers.

You may implement the service in **Goâ€¯1.22+ or Pythonâ€¯3.11+**. Mixing in other languages or frameworks for tooling is fine if it helps.

---

## ğŸŒŸ Niceâ€‘toâ€‘have Enhancements

* Autoâ€‘generate a concise, humanâ€‘readable title for every new chat.  
* Allow users to give thumbsâ€‘up / thumbsâ€‘down feedback on any agent message.  
* Log tool invocations and surface simple analytics (e.g., % of questions resolved without web search).  
* Containerisation (Docker) or a dev script (`make`, `task`, `invoke`, etc.) for oneâ€‘command setup.  
* Deployment notes for a cloud environment of your choice.  
* AIâ€‘generated simple UI (100â€¯% optional).

---

## ğŸš€ Getting Your Solution Running

Provide a short guide (README section or shell script) that covers:

1. **Startup** â€“ how to launch the service (and any supporting services such as a vector DB).  
2. **Configuration** â€“ environment variables, API keys, or model endpoints needed.  
3. **Testing** â€“ how to run unit and integration tests.  
4. **Assumptions** â€“ anything nonâ€‘obvious you decided (e.g., which LLM provider you picked, why a specific vector DB).

---

## ğŸ§© Submission

Fork this repository, commit your solution to your fork, and send it back to us.

Good luck, potential colleagueâ€”may the (reinforcement) learning be with you!
